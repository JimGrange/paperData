eBAC(g = 181.6, 181, 79.3, 6)
3 / 3*3
3 / (3*3)
3-9
3/9
install.packages("trimr")
install.packages("trimr")
install.packages("trimr")
install.packages("trimr")
install.packages("trimr")
install.packages("trimr")
install.packages("taber")
install.packages("repo")
install.packages("trimr")
install.packages("repo")
install.packages("trimr")
install.packages("mixAK")
install.packages("pixiedust")
y <- rnorm(100, mean = 105, sd = 20)
t.test(x, y, paired = TRUE)
library(ggplot2)
citation("ggplot2")
citation("trimr")
citation("ez")
citation("retimes")
install.packages("retimes")
citation("retimes")
rwiener
citation("rwiener")
citation("rwiener")
install.packages("rwiener")
install.packages("Rwiener")
install.packages("RWiener")
citation("RWiener")
install.packages("boot")
boot.ci(results, type="bca")
install.packages("rms")
library(rms)
x=c(1,2,3,2,3,1,2,3,3,3,2,2,1,2,1,2,3,2,1,2)
y=c("math","eco","eco","lit","lit","eco","eco","math","math","lit","lit","math","eco","eco","math","lit","lit","math","eco","math")
Dataset<-data.frame(x,y)
h <- orm(x ~ y)
h
# Bootstrap 95% CI for R-Squared
library(boot)
# function to obtain R-Squared from the data
rsq <- function(formula, data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- orm(x ~ y, data=data[indices,])
return((fit)$r.square)
}
# bootstrapping with 1000 replications
results <- boot(data=Dataset, statistic=rsq, R=1000, formula=x ~ y)
# view results
results
plot(results)
# get 95% confidence interval
boot.ci(results, type="bca")
rsq <- function(formula, data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- orm(x ~ y, data=data[indices,])
return(fit$r.square)
}
# bootstrapping with 1000 replications
results <- boot(data=Dataset, statistic=rsq, R=1000, formula=x ~ y)
# view results
results
plot(results)
# get 95% confidence interval
boot.ci(results, type="bca")
install.packages("DSuR")
aaply(1:10000, 1, function(x) rnorm(1, mean=x), .parallel=TRUE)
aply(1:10000, 1, function(x) rnorm(1, mean=x), .parallel=TRUE)
apply(1:10000, 1, function(x) rnorm(1, mean=x), .parallel=TRUE)
2 + 2
3 * 3
x <- 3
y <- 4
x + y
devtools::install_github("JimGrange/flankr")
library(flankr)
?fitDSTP
data(exampleData)
fit <- fitDSTP(data = exampleData, conditionName = "present", nTrials = 200)
plotFitDSTP(fit, exampleData, conditionName = "present")
?cumsum
cumsum(1:2)
cumsum(1:5)
sim_single_trial <- function
(
strength=0,           #strength of signal
bias=0,               #constant added to drift (parameter)
startingpoint=0,      #another bias parameter
noise=1,              #how much variance accumulates per unit time (>0)
bound=1,              #how high the bounds on the accumulator (>0)
timestep=0.02,        #how small the timestep
ndt = 0.2,            #residual decision/reaction time (>0)
fuzz = 0.05,          #response time fuzz (to hide timesteps) (>0)
nsteps=250)
{
decision_variable <- cumsum(rnorm(n=nsteps,
mean=(bias + strength)*timestep,
sd=sqrt(noise) * sqrt(timestep))) + startingpoint
escape_step <- which(abs(decision_variable) > bound)[1]
c(rt=ndt + escape_step * timestep + rnorm(1, sd=fuzz),
response = decision_variable[escape_step] > 0)
}
sim <- function
(
strength=0,           #strength of signal
bias=0,               #constant added to drift (parameter)
startingpoint=0,      #another bias parameter
noise=1,              #how much variance accumulates per unit time (>0)
bound=1,              #how high the bounds on the accumulator (>0)
timestep=0.02,        #how small the timestep
ndt = 0.2,            #residual decision/reaction time (>0)
fuzz = 0.05,          #response time fuzz (to hide timesteps) (>0)
nsteps=250)
{
decision_variable <- cumsum(rnorm(n=nsteps,
mean=(bias + strength)*timestep,
sd=sqrt(noise) * sqrt(timestep))) + startingpoint
escape_step <- which(abs(decision_variable) > bound)[1]
c(rt=ndt + escape_step * timestep + rnorm(1, sd=fuzz),
response = decision_variable[escape_step] > 0)
}
sim()
sim(strength = 0.3)
sim(strength = 0.3)
sim(strength = 0.3)
sim(strength = 0.3)
sim(strength = 0.3)
sim(strength = 0.3)
sim(strength = 0.3)
sim(strength = 0.3)
sim(strength = 0.3)
strength=0,           #strength of signal
bias=0,               #constant added to drift (parameter)
startingpoint=0,      #another bias parameter
noise=1,              #how much variance accumulates per unit time (>0)
bound=1,              #how high the bounds on the accumulator (>0)
timestep=0.02,        #how small the timestep
ndt = 0.2,            #residual decision/reaction time (>0)
fuzz = 0.05,          #response time fuzz (to hide timesteps) (>0)
nsteps=250)
strength=0
bias=0
startingpoint=0
noise=1
bound=1
timestep=0.02
ndt = 0.2
fuzz = 0.05
nsteps=250
decision_variable <- cumsum(rnorm(n=nsteps,
mean=(bias + strength)*timestep,
sd=sqrt(noise) * sqrt(timestep))) + startingpoint
decision_variable
plot(decision_variable)
plot(decision_variable, type = "l")
which(abs(decision_variable) > bound)[1]
escape_step <- which(abs(decision_variable) > bound)[1]
decision_variable[104]
decision_variable[103]
decision_variable[105]
escape_step * timestep
rnorm(1, sd=fuzz)
rt=ndt + escape_step * timestep + rnorm(1, sd=fuzz)
rt
decision_variable[escape_step] > 0
response = decision_variable[escape_step] > 0
response
c(rt=ndt + escape_step * timestep + rnorm(1, sd=fuzz),
response = decision_variable[escape_step] > 0)
strengths <- as.vector((2^(10:15) / 10000) %o% c(-1, 1))
strengths
rt.data <- rdply(1000, cbind(strength=strengths, ldply(strengths, sim_single_trial)))
library(plyr)
rt.data <- rdply(1000, cbind(strength=strengths, ldply(strengths, sim_single_trial)))
rt.data
head(rt.data)
library(ggplot2)
library(plyr)
theme_set(theme_bw())
theme_update(panel.border=element_blank())
# This function simulates a single reaction time trial, producing a named vector
# of the reaction time and the binary response.
sim_single_trial <- function
(
strength=0,           #strength of signal
bias=0,               #constant added to drift (parameter)
startingpoint=0,      #another bias parameter
noise=1,              #how much variance accumulates per unit time (>0)
bound=1,              #how high the bounds on the accumulator (>0)
timestep=0.02,        #how small the timestep
ndt = 0.2,            #residual decision/reaction time (>0)
fuzz = 0.05,          #response time fuzz (to hide timesteps) (>0)
nsteps=250)
{
decision_variable <- cumsum(rnorm(n=nsteps,
mean=(bias + strength)*timestep,
sd=sqrt(noise) * sqrt(timestep))) + startingpoint
escape_step <- which(abs(decision_variable) > bound)[1]
c(rt=ndt + escape_step * timestep + rnorm(1, sd=fuzz),
response = decision_variable[escape_step] > 0)
}
sim_single_trial(strength=0.3)
#"strength" is the strength of the signal in the stimulus; a typical
#experiment will use a mixture of strengths, positive and negative,
#Here are the strengths we will use:
strengths <- as.vector((2^(10:15) / 10000) %o% c(-1, 1))
strengths
#Simulate 1000 trials at each strength.
rt.data <- rdply(1000, cbind(strength=strengths, ldply(strengths, sim_single_trial)))
rt.data$.n <- NULL
head(rt.data)
rt.data <- subset(rt.data, !(is.na(rt) | rt > 4))
rt,data()
rt.data[sample(nrow(rt.data), 10),]
fold <- function(data)
mutate(data,
response = xor(response, strength<0),
strength = abs(strength))
bernoulli_mean_ci <- function(x) {
conf = prop.test(sum(as.logical(x)), length(x))$conf.int
data.frame(y = mean(x), ymin = conf[1], ymax = conf[2])
}
psi.plot <- (
ggplot(fold(rt.data))
+ aes(x=strength, y=as.numeric(response))
+ stat_summary(fun.data=bernoulli_mean_ci, geom="pointrange")
+ labs(x="Coherence", y="Proportion correct")
+ geom_hline(y=0.5) + geom_vline(x=0)
+ geom_smooth(method="glm", formula=y~x-1,
family=binomial(link=logit)))
print(psi.plot)
rt.plot <- (
ggplot(fold(rt.data)) + aes(x=rt)
+ stat_density(position="identity", fill="NA", geom="line")
+ labs(x="Response time", y="Density", title="Response time distribution")
+ theme(axis.text.y = element_blank())
+ geom_hline(y=0)
)
print(rt.plot)
slow.error.plot <- (
rt.plot
+ aes(color=factor(response))
+ labs(color="Response\ncorrect"))
print(slow.error.plot)
rt.strength.plot <- (
ggplot(fold(rt.data))
+ aes(rt, color=factor(strength))
+ stat_density(geom="line", position="identity")
+ scale_color_discrete("Strength", h=c(270,0), direction=1)
+ labs(x="Response time", y="Density",
title="Distribution of response times by stimulus strength")
)
print(rt.strength.plot)
(ggplot(fold(rt.data)) + aes(x=factor(strength), y=rt, color=response)
+ stat_summary(fun.data=mean_cl_boot, geom="pointrange",
position=position_dodge(width=0.25))
+ labs(title="RTs conditional on correct answer and stimulus strength",
x="Stimulus strength",
y="Response time",
color="Response\ncorrect"))
m = .9
p = .6
f = 400
s = 1000
p*f + (1-p)*s
p = 0.7
f = 400
s = 600
n = 10000
data <- numeric(n)
data
runif(1, 0, 1)
for(i in 1:n){
num <- runif(1, 0, 1)
if(num < p){
data[i] <- f
} else {
data[i] <- s
}
}
data
mean(data)
?switch
?mean
mean(c(2.13, 2.86, 2.86, 3.37, 2.93, 2.86, 9.96, 2.86, 2.22, 2.99, 2.13, 2.86, 2.93, 3.27, 33.61))
median(c(2.13, 2.86, 2.86, 3.37, 2.93, 2.86, 9.96, 2.86, 2.22, 2.99, 2.13, 2.86, 2.93, 3.27, 33.61))
sd(c(2.13, 2.86, 2.86, 3.37, 2.93, 2.86, 9.96, 2.86, 2.22, 2.99, 2.13, 2.86, 2.93, 3.27, 33.61))
324/6
9/6
9/5
23/6
library(shiny)
runGitHub("shiny_example", "rstudio")
runGitHub("shiny_example", "rstudio")
runGitHub("shiny_example", "rstudio", subdir = "inst/shinyapp/")
runGitHub("shiny_example", "rstudio", subdir = "inst/shinyapp/")
shiny::runGitHub("shiny-examples", "rstudio", subdir = "001-hello")
shiny::runGitHub("shiny-examples", "rstudio", subdir = "082-word-cloud")
shiny::runGitHub("shiny-examples", "rstudio", subdir = "004-mpg")
x <- seq(0, 1, length.out = 10000)
plot(x, dnorm(x, mean = 0.5, sd = 0.1))
plot(x, pnorm(x, mean = 0.5, sd = 0.1))
integrate(dnorm, lower = 0, upper = 0.6, mean = 0.5, sd = 0.1)
x <- seq(0, 1, length.out = 10000)
par(mfrow = c(1, 2))
plot(x, dnorm(x, mean = 0.5, sd = 0.1))
plot(x, pnorm(x, mean = 0.5, sd = 0.1))
integrate(dnorm, lower = 0, upper = 0.6, mean = 0.5, sd = 0.1)
x <- seq(0, 1, length.out = 10000)
par(mfrow = c(1, 2))
plot(x, dnorm(x, mean = 0.5, sd = 0.1))
plot(x, pnorm(x, mean = 0.5, sd = 0.1))
abline(v = 0.6)
integrate(dnorm, lower = 0, upper = 0.6, mean = 0.5, sd = 0.1)
z <- dnorm(x, mean = 0.5, sd = 0.1)
y <- pnorm(x, mea = 0.5, sd = 0.1)
z
y
which(y == .6)
which(y == 0.6)[[1]]
match(0.6, y)
which(0.6 == y)[[1]]
which(>0.6 == y)[[1]]
which(0.6 > y)[[1]]
y[1]
which(0.6 < y)[[1]]
y[5254]
z[5254]
y[5254]
y
x <- seq(0, 1, length.out = 10000)
z <- dnorm(x, mean = 0.5, sd = 0.1)
y <- pnorm(x, mea = 0.5, sd = 0.1)
a <- which(0.8 < y)[[1]]
par(mfrow = c(1, 2))
plot(x, z)
plot(x, y)
abline(h = 0.8, v = a)
a
x <- seq(0, 1, length.out = 10000)
z <- dnorm(x, mean = 0.5, sd = 0.1)
y <- pnorm(x, mea = 0.5, sd = 0.1)
a <- which(0.8 < y)[[1]]
par(mfrow = c(1, 2))
plot(x, z)
plot(x, y)
abline(h = 0.8, v = x[a])
integrate(dnorm, lower = 0, upper = x[a], mean = 0.5, sd = 0.1)
x <- seq(0, 1, length.out = 10000)
z <- dnorm(x, mean = 0.5, sd = 0.1)
y <- pnorm(x, mea = 0.5, sd = 0.1)
a <- which(0.8 < y)[[1]]
# plot the PDF (left) and CDF (right)
par(mfrow = c(1, 2))
plot(x, z, type = "l", ylab = PDF(x))
plot(x, y, type = "l", ylab = CDF(x))
abline(h = 0.8, v = x[a])
integrate(dnorm, lower = 0, upper = x[a], mean = 0.5, sd = 0.1)
x <- seq(0, 1, length.out = 10000)
z <- dnorm(x, mean = 0.5, sd = 0.1)
y <- pnorm(x, mea = 0.5, sd = 0.1)
a <- which(0.8 < y)[[1]]
# plot the PDF (left) and CDF (right)
par(mfrow = c(1, 2))
plot(x, z, type = "l", ylab = "PDF(x)")
plot(x, y, type = "l", ylab = "CDF(x)")
abline(h = 0.8, v = x[a])
integrate(dnorm, lower = 0, upper = x[a], mean = 0.5, sd = 0.1)
x <- seq(0, 1, length.out = 10000)
z <- dnorm(x, mean = 0.5, sd = 0.1)
y <- pnorm(x, mea = 0.5, sd = 0.1)
a <- which(0.8 < y)[[1]]
# plot the PDF (left) and CDF (right)
par(mfrow = c(1, 2))
plot(x, z, type = "l", ylab = "PDF(x)")
plot(x, y, type = "l", ylab = "CDF(x)")
abline(h = 0.8, v = x[a], col = "red")
integrate(dnorm, lower = 0, upper = x[a], mean = 0.5, sd = 0.1)
which(0.6 < x)[[1]]
x <- seq(0, 1, length.out = 10000)
z <- dnorm(x, mean = 0.5, sd = 0.1)
y <- pnorm(x, mea = 0.5, sd = 0.1)
a <- which(0.6 < x)[[1]]
# plot the PDF (left) and CDF (right)
par(mfrow = c(1, 2))
plot(x, z, type = "l", ylab = "PDF(x)")
plot(x, y, type = "l", ylab = "CDF(x)")
abline(h = y[a], v = x[a], col = "red")
integrate(dnorm, lower = 0, upper = x[a], mean = 0.5, sd = 0.1)
y[a]
log(0.01)
log(0.1)
log(1)
log(1.1)
log(1.001)
5 add 4
rm(list = ls())
setwd("~/Git/paperData/2015/Mixture Distribution/Testing Mixture Distributions")
source("functions.R")
library(dplyr)
# get the data (& do trimming)
data <- read.csv("data.csv")
data$rt <- round(data$rt / 1000, 3)
data = sdTrim(data, minRT = 0.150, sd = 2.5, perCondition = FALSE,
perParticipant = TRUE,  returnType = "raw")
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
### do the mixture plots
# define ratios to include in the analysis
ratios <- c(0.05, 1, 20)
ratioData <- subset(data, data$ratio %in% ratios)
# subset of data to pass to fp package. Then, change names to match
# fp package function requirements
ratioData <- select(ratioData, rt, ratio, participant)
colnames(ratioData) <- c("rt", "cond", "pp")
# get the fp1 object
res <- fpGet(ratioData[, c("rt", "cond")], 1000, bw = 0.75)
# call plot
par(las = 1, mfrow = c(1, 3), mar = c(4.2, 4.2, 1, 1))
plot(res, xlab = "Resonse Time (S)", col = 1)
# do Bayesian analysis
res <- tapply(1:nrow(ratioData), ratioData$pp, function(X) {
fpGet(ratioData[X, ], 1000, bw = 0.75)
})  # compute the list
fpAnova(res, stat = "both")
# do box-plot
crosses = fpDensDiff(res)  # first, get the crossing points
boxplot(t(crosses), frame.plot = F, xlab = "Crossing point",
ylab = "Condition pair", names = c("3-2", "2-1", "3-1"),
horizontal = T)
#------------------------------------------------------------------------------
setwd("~/Git/paperData/2015/Temporal Distinctiveness & Mixture Distributions/Data & Model Code/Testing Mixture Distributions/fp Data")
rm(list = ls())
setwd("~/Git/paperData/2015/Temporal Distinctiveness & Mixture Distributions/Data & Model Code/Testing Mixture Distributions/fp Data")
source("functions.R")
library(dplyr)
# get the data (& do trimming)
data <- read.csv("data.csv")
data$rt <- round(data$rt / 1000, 3)
data = sdTrim(data, minRT = 0.150, sd = 2.5, perCondition = FALSE,
perParticipant = TRUE,  returnType = "raw")
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
### do the mixture plots
# define ratios to include in the analysis
ratios <- c(0.05, 1, 20)
ratioData <- subset(data, data$ratio %in% ratios)
# subset of data to pass to fp package. Then, change names to match
# fp package function requirements
ratioData <- select(ratioData, rt, ratio, participant)
colnames(ratioData) <- c("rt", "cond", "pp")
# get the fp1 object
res <- fpGet(ratioData[, c("rt", "cond")], 1000, bw = 0.75)
# call plot
par(las = 1, mfrow = c(1, 3), mar = c(4.2, 4.2, 1, 1))
plot(res, xlab = "Resonse Time (S)", col = 1)
# do Bayesian analysis
res <- tapply(1:nrow(ratioData), ratioData$pp, function(X) {
fpGet(ratioData[X, ], 1000, bw = 0.75)
})  # compute the list
fpAnova(res, stat = "both")
# do box-plot
crosses = fpDensDiff(res)  # first, get the crossing points
boxplot(t(crosses), frame.plot = F, xlab = "Crossing point",
ylab = "Condition pair", names = c("3-2", "2-1", "3-1"),
horizontal = T)
#------------------------------------------------------------------------------
res$p
res
str(res)
# do box-plot
rm(list = ls())
setwd("~/Git/paperData/2015/Temporal Distinctiveness & Mixture Distributions/Data & Model Code/Testing Mixture Distributions/fp Data")
source("functions.R")
library(dplyr)
# get the data (& do trimming)
data <- read.csv("data.csv")
data$rt <- round(data$rt / 1000, 3)
data = sdTrim(data, minRT = 0.150, sd = 2.5, perCondition = FALSE,
perParticipant = TRUE,  returnType = "raw")
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
### do the mixture plots
# define ratios to include in the analysis
ratios <- c(0.05, 1, 20)
ratioData <- subset(data, data$ratio %in% ratios)
# subset of data to pass to fp package. Then, change names to match
# fp package function requirements
ratioData <- select(ratioData, rt, ratio, participant)
colnames(ratioData) <- c("rt", "cond", "pp")
# get the fp1 object
res <- fpGet(ratioData[, c("rt", "cond")], 1000, bw = 0.75)
# call plot
par(las = 1, mfrow = c(1, 3), mar = c(4.2, 4.2, 1, 1))
plot(res, xlab = "Resonse Time (S)", col = 1)
# do Bayesian analysis
res <- tapply(1:nrow(ratioData), ratioData$pp, function(X) {
fpGet(ratioData[X, ], 1000, bw = 0.75)
})  # compute the list
fpAnova(res, stat = "both")
# do box-plot
crosses = fpDensDiff(res)  # first, get the crossing points
boxplot(t(crosses), frame.plot = F, xlab = "Crossing point",
ylab = "Condition pair", names = c("3-2", "2-1", "3-1"),
horizontal = T)
#------------------------------------------------------------------------------
1 / 0.298125
